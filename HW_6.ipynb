{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Mh8KqXb0yR8",
        "colab_type": "text"
      },
      "source": [
        "# Homework 6: Neural Coreference Resolution\n",
        "\n",
        "**Due April 20, 2020 at 11:59pm**\n",
        "\n",
        "In this homework,  you will be implementing parts of a Pytorch implementation for neural coreference resolution, inspired by [Lee et al.(2017), “End-to-end Neural Coreference Resolution” (EMNLP)](https://arxiv.org/pdf/1707.07045.pdf). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UqglTYOZ9jd",
        "colab_type": "text"
      },
      "source": [
        "### REMEMBER TO UPLOAD THE DATASET!\n",
        "Click the Files icon > Upload > Upload train.conll and dev.conll that you have downloaded from bCourses: Files/HW_6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpxETEnm1c0h",
        "colab_type": "text"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7u1xOC_zcEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys, re\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "from scipy.stats import spearmanr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1Wm71YOajw0",
        "colab_type": "text"
      },
      "source": [
        "We noticed that running this on CPU is faster than running on GPU. Thus, we will default to running on CPU. However, feel free to change it to GPU if you wish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5qiaj6zzfSN",
        "colab_type": "code",
        "outputId": "e3a6b1c8-07f5-497d-97ef-573cb31a4adc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cpu\")\n",
        "print(\"Running on {}\".format(device))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qTmWIJq56IA",
        "colab_type": "text"
      },
      "source": [
        "### Download and process data\n",
        "Note: You do **not** have to modify this section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDP_KFPo7N12",
        "colab_type": "code",
        "outputId": "74b12922-57fc-4fb8-e22d-d61619b20099",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-20 17:55:12--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-04-20 17:55:13--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-04-20 17:55:13--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.11MB/s    in 6m 31s  \n",
            "\n",
            "2020-04-20 18:01:45 (2.10 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRDDk-tu-EfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_conll(filename):\n",
        "\n",
        "  docid=None\n",
        "  partID=None\n",
        "\n",
        "  # collection\n",
        "  all_sents=[]\n",
        "  all_ents=[]\n",
        "\n",
        "  # for one doc\n",
        "  all_doc_sents=[]\n",
        "  all_doc_ents=[]\n",
        "\n",
        "  # for one sentence\n",
        "  sent=[]\n",
        "  ents=[]\n",
        "\n",
        "  named_ents=[]\n",
        "  cur_tid=0\n",
        "  open_count=0\n",
        "\n",
        "  global_eid=0\n",
        "  doc_eid_to_global_eid={}\n",
        "\n",
        "  with open(filename, encoding=\"utf-8\") as file:\n",
        "    for line in file:\n",
        "      if line.startswith(\"#begin document\"):\n",
        "\n",
        "        all_doc_ents=[]\n",
        "        all_doc_sents=[]\n",
        "\n",
        "        open_ents={}\n",
        "        open_named_ents={}\n",
        "\n",
        "        docid=None\n",
        "        matcher=re.match(\"#begin document \\((.*)\\); part (.*)$\", line.rstrip())\n",
        "        if matcher != None:\n",
        "          docid=matcher.group(1)\n",
        "          partID=matcher.group(2)\n",
        "\n",
        "      elif line.startswith(\"#end document\"):\n",
        "\n",
        "        all_sents.append(all_doc_sents)\n",
        "        all_ents.append(all_doc_ents)\n",
        "\n",
        "        \n",
        "      else:\n",
        "\n",
        "        parts=re.split(\"\\s+\", line.rstrip())\n",
        "\n",
        "        # sentence boundary\n",
        "        if len(parts) < 2:\n",
        "    \n",
        "          all_doc_sents.append(sent)\n",
        "\n",
        "          ents=sorted(ents, key=lambda x: (x[0], x[1]))\n",
        "\n",
        "          all_doc_ents.append(ents)\n",
        "\n",
        "          sent=[]\n",
        "          ents=[]\n",
        "\n",
        "          cur_tid=0\n",
        "\n",
        "          continue\n",
        "\n",
        "        tid=cur_tid\n",
        "        token=parts[3]\n",
        "        cur_tid+=1\n",
        "\n",
        "        identifier=\"%s.%s\" % (docid, partID)\n",
        "\n",
        "        coref=parts[-1].split(\"|\")\n",
        "\n",
        "        for c in coref:\n",
        "          if c.startswith(\"(\") and c.endswith(\")\"):\n",
        "            c=re.sub(\"\\(\", \"\", c)\n",
        "            c=int(re.sub(\"\\)\", \"\", c))\n",
        "\n",
        "            if (identifier, c) not in doc_eid_to_global_eid:\n",
        "              doc_eid_to_global_eid[(identifier, c)]=len(doc_eid_to_global_eid)\n",
        "\n",
        "            ents.append((tid, tid, doc_eid_to_global_eid[(identifier, c)], identifier))\n",
        "\n",
        "          elif c.startswith(\"(\"):\n",
        "            c=int(re.sub(\"\\(\", \"\", c))\n",
        "\n",
        "            if c not in open_ents:\n",
        "              open_ents[c]=[]\n",
        "            open_ents[c].append(tid)\n",
        "            open_count+=1\n",
        "\n",
        "          elif c.endswith(\")\"):\n",
        "            c=int(re.sub(\"\\)\", \"\", c))\n",
        "\n",
        "            assert c in open_ents\n",
        "\n",
        "            start_tid=open_ents[c].pop()\n",
        "            open_count-=1\n",
        "\n",
        "            if (identifier, c) not in doc_eid_to_global_eid:\n",
        "              doc_eid_to_global_eid[(identifier, c)]=len(doc_eid_to_global_eid)\n",
        "\n",
        "            ents.append((start_tid, tid, doc_eid_to_global_eid[(identifier, c)], identifier))\n",
        "\n",
        "        sent.append(token)\n",
        "\n",
        "  return all_sents, all_ents\n",
        "\n",
        "def load_embeddings(filename, vocab_size):\n",
        "  # 0 idx is for padding\n",
        "  # 1 idx is for unknown words\n",
        "\n",
        "  # get the embedding size from the first embedding\n",
        "  with open(filename, encoding=\"utf-8\") as file:\n",
        "    word_embedding_dim=len(file.readline().split(\" \"))-1\n",
        "\n",
        "  vocab={\"[PAD]\":0, \"[UNK]\":1}\n",
        "\n",
        "  print(\"word_embedding_dim:\", word_embedding_dim)\n",
        "\n",
        "  embeddings=np.zeros((vocab_size, word_embedding_dim))\n",
        "\n",
        "  with open(filename, encoding=\"utf-8\") as file:\n",
        "    for idx,line in enumerate(file):\n",
        "\n",
        "      if idx + 2 >= vocab_size:\n",
        "        break\n",
        "\n",
        "      cols=line.rstrip().split(\" \")\n",
        "      val=np.array(cols[1:])\n",
        "      word=cols[0]\n",
        "      embeddings[idx+2]=val\n",
        "      vocab[word]=idx+2\n",
        "\n",
        "  return torch.FloatTensor(embeddings), vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F17zKOXO6Cap",
        "colab_type": "code",
        "outputId": "7425ec9e-661b-4a91-835f-421a7bcc98a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "embeddingFile = \"glove.6B.50d.txt\"\n",
        "trainFile = \"train.conll\"\n",
        "devFile = \"dev.conll\"\n",
        "\n",
        "all_sents, all_ents=read_conll(trainFile)\t\n",
        "dev_all_sents, dev_all_ents=read_conll(devFile)\n",
        "\n",
        "embeddings, vocab=load_embeddings(embeddingFile, 50000)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word_embedding_dim: 50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeMhT6UN1eP5",
        "colab_type": "text"
      },
      "source": [
        "### **Part 1. Implement B3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YshAEm0xd8NL",
        "colab_type": "text"
      },
      "source": [
        "In this part, you’ll implement the B3 coreference metric as discussed in class without importing external libraries. \n",
        "\n",
        "Recall the definition: \n",
        "$B^{_{precision}^{3}} = \\frac{1}{n}\\sum_{i}^{n} \\frac{\\left |Gold_{i} \\cap  System_{i} \\right |}{\\left | System_{i} \\right |}$\n",
        "$B^{_{recall}^{3}} = \\frac{1}{n}\\sum_{i}^{n} \\frac{\\left |Gold_{i} \\cap  System_{i} \\right |}{\\left | Gold_{i} \\right |}$\n",
        "\n",
        "You should be able to pass the sanity check b3_test() after implementing it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpEJaiqez-bd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def b3(gold, system):\n",
        "  \"\"\" Calculate B3 metrics given the gold and system output\n",
        "    Args:\n",
        "        gold  : A dictionary that contains true references. The key is a tuple, (docid, absolute_start_idx, absolute_end_idx)\n",
        "                representing a target to be predicted; the value is the true reference entity id.\n",
        "        system: A dictionary that contains predicted reference. The key in gold and system should be identical; the value\n",
        "                is the predicted entity generated by the model.\n",
        "    Returns:\n",
        "        precision, recall, F(following the formula above)\n",
        "\n",
        "    \"\"\"\n",
        "  precision=0.\n",
        "  recall=0.\n",
        "  F = 0.\n",
        "  #####\n",
        "  # make a dictionary. maps list of words to group \n",
        "  words_to_group = {}\n",
        "  temp_gold = {}\n",
        "  temp_sys = {}\n",
        "  for key, value in gold.items():\n",
        "    if value not in words_to_group.keys():\n",
        "      words_to_group[value] = []\n",
        "    words_to_group[value].append(key)\n",
        "  for key, value in gold.items():\n",
        "    temp_gold[key] = words_to_group[value]\n",
        "\n",
        "  words_to_group.clear()\n",
        "  for key, value in system.items():\n",
        "    if value not in words_to_group.keys():\n",
        "      words_to_group[value] = []  \n",
        "    words_to_group[value].append(key)\n",
        "  for key, value in system.items():\n",
        "    temp_sys[key] = words_to_group[value]  \n",
        "\n",
        "  # iterate over gold once\n",
        "  # find groups with the same values in gold, in system, two groups per word\n",
        "  # temp_gold = {} \n",
        "  # temp_sys = {}\n",
        "  # for key1, value1 in gold.items():\n",
        "  #   for key2, value2 in gold.items(): \n",
        "  #     if key1 not in temp_gold:\n",
        "  #       temp_gold[key1] = []\n",
        "  #     if value1 == value2:\n",
        "  #       temp_gold[key1].append(key2)\n",
        "  # for key1, value1 in system.items():\n",
        "  #   for key2, value2 in system.items(): \n",
        "  #     if key1 not in temp_sys:\n",
        "  #       temp_sys[key1] = []\n",
        "  #     if value1 == value2:\n",
        "  #       temp_sys[key1].append(key2)\n",
        "  \n",
        "  # for each word, find the key intersection between the two groups, numerator term\n",
        "  intersect = {} \n",
        "  for word in gold.keys():\n",
        "    set_gold = set(temp_gold[word])\n",
        "    set_sys = set(temp_sys[word])\n",
        "    inter_temp = set_gold & set_sys\n",
        "    intersect[word] = len(inter_temp)\n",
        "  \n",
        "  # denominator is generated by group size in gold (for recall)\n",
        "  # denominator is generated by group size in system (for precision)\n",
        "  for word in intersect.keys():\n",
        "    p_fract = intersect[word]/len(temp_sys[word])\n",
        "    r_fract = intersect[word]/len(temp_gold[word])\n",
        "    precision += p_fract\n",
        "    recall += r_fract\n",
        "\n",
        "  precision = (1/len(gold))*precision\n",
        "  recall = (1/len(system))*recall\n",
        "  \n",
        "  f_num = 2*(precision*recall)\n",
        "  f_denom = (precision+recall)\n",
        "  F = f_num/f_denom\n",
        "\n",
        "  #####\n",
        "\n",
        "  return precision, recall, F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eybIuitz3Co",
        "colab_type": "code",
        "outputId": "87d6f906-a5b1-424c-87a8-508547253339",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "def b3_test():\n",
        "  # for a1, we have 5 mentions of the same entity (?) abs(gold1) = 5, abs(system1) = 0, abs(gold2) = 3\n",
        "  # precision and recall will be 0 for the sanity check\n",
        "\n",
        "  #abs(golda1) = 5 --> abs(systema1) = 4\n",
        "  # intersection is how many mentions are in common \n",
        "  # key: \"word\", value: the entity it refers to\n",
        "  # a1, a3, a9, a10 referencing same thing (all stuff w/ value 1)\n",
        "  # for word a1, the intersection is a1 and a9 because they are in the same group relative to the word a1. we do this for each word\n",
        "  gold={\"a1\":1, \"a2\": 2, \"a3\": 1, \"a4\":1, \"a5\": 3, \"a6\":3, \"a7\":2, \"a8\":2, \"a9\":1, \"a10\":1}\n",
        "  system={\"a1\":5, \"a2\": 6, \"a3\": 6, \"a4\":6, \"a5\": 7, \"a6\":7, \"a7\":5, \"a8\":5, \"a9\":5, \"a10\":8}\n",
        "\n",
        "  precision, recall, F=b3(gold, system)\n",
        "  print(\"P: %.3f, R: %.3f, F: %.3f\" % (precision, recall, F))\n",
        "\n",
        "  assert abs(precision-0.667) < 0.001\n",
        "  assert abs(recall-0.547) < 0.001\n",
        "  assert abs(F-0.601) < 0.001\n",
        "  \n",
        "  print (\"B3 sanity check passed\")\n",
        "b3_test()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "P: 0.667, R: 0.547, F: 0.601\n",
            "B3 sanity check passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNFaD5IO_P3F",
        "colab_type": "text"
      },
      "source": [
        "### **Part 2. Neural coref**\n",
        "In part 2, the skeleton code for mention-ranking model is provided to you, you will not need to change any code until Part 2.1 begins. The following section provides the Mention class which is used to store relavant information about a mention and the BasicCorefModel. You will, at the very least, need to carefully read these two classes and understand the information stored in Mention and the structure of the model to complete this homework.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dVP6JsQzghb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mention():\n",
        "\n",
        "  \"\"\"\n",
        "  An object to contain information about each mention\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, mention_id, docid, absolute_start_idx, absolute_end_idx, sentence_start_idx, sentence_end_idx, sentence, vocab):\n",
        "    self.docid=docid\n",
        "\n",
        "    # mention id (globally unique within one file, but not across different train and test files)\n",
        "    self.mention_id=mention_id\n",
        "    # the token index of the mention start position, measured from the beginning of the document\n",
        "    self.absolute_start_idx=absolute_start_idx\n",
        "    # the token index of the mention end position, measured from the beginning of the document\n",
        "    self.absolute_end_idx=absolute_end_idx\n",
        "    # the token index of the mention start position, measured from the beginning of the sentence\n",
        "    self.sentence_start_idx=sentence_start_idx\n",
        "    # the token index of the mention end position, measured from the beginning of the sentence\n",
        "    self.sentence_end_idx=sentence_end_idx\n",
        "    # a list of tokens for all the words in the mention's sentence\n",
        "    self.sentence=sentence\n",
        "    # a list of tokens ids for all the words in the mention's sentence\n",
        "    self.sentence_ids=[]\n",
        "    self.sentence_length=len(sentence)\n",
        "\n",
        "    for word in sentence:\n",
        "      word=word.lower()\n",
        "      self.sentence_ids.append(vocab[word] if word in vocab else vocab[\"[UNK]\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQ3B8CU4AGvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_data_to_training_instances(all_sents, all_ents, vocab):\n",
        "  X=[]\n",
        "  Y=[]\n",
        "  M=[]\n",
        "\n",
        "  global_id=0\n",
        "  truth={}\n",
        "\n",
        "  for doc_idx, doc_ent in enumerate(all_ents):\n",
        "\n",
        "    current_token_position=0\n",
        "\n",
        "    existing_mentions=[]\n",
        "\n",
        "    for sent_idx, mention_list in enumerate(doc_ent):\n",
        "      sent=all_sents[doc_idx][sent_idx]\n",
        "\n",
        "      for mention_idx, mention in enumerate(mention_list):\n",
        "\n",
        "        start_sent_idx, end_sent_idx, entity_id, identifier=mention\n",
        "\n",
        "        mention=Mention(global_id, identifier, current_token_position+start_sent_idx, current_token_position+end_sent_idx, start_sent_idx, end_sent_idx, sent, vocab)\n",
        "        M.append(mention)\n",
        "        truth[global_id]=entity_id\n",
        "\n",
        "        global_id+=1\n",
        "\n",
        "        x=[]\n",
        "        y=[]\n",
        "\n",
        "        for aidx, antecedent in enumerate(existing_mentions):\n",
        "          x.append(antecedent)\n",
        "          if truth[antecedent.mention_id] == truth[mention.mention_id]:\n",
        "            y.append(aidx)\n",
        "\n",
        "        X.append(x)\n",
        "        Y.append(torch.LongTensor(y).to(device))\n",
        "\n",
        "        existing_mentions.append(mention)\n",
        "\n",
        "      current_token_position+=len(sent)\n",
        "\n",
        "  return X, Y, M, truth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYmOCaewzj3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BasicCorefModel(nn.Module):\n",
        "\n",
        "\tdef __init__(self, vocab, embeddings):\n",
        "\t\tsuper(BasicCorefModel, self).__init__()\n",
        "\n",
        "\t\tself.vocab=vocab\n",
        "\n",
        "\t\tself.embeddings = nn.Embedding.from_pretrained(embeddings)\n",
        "\t\n",
        "\t\t#self.d_embeddings = nn.Embedding.from_pretrained(embeddings)\n",
        "\n",
        "\t\t_, embedding_size=embeddings.shape\n",
        "\n",
        "\t\tself.hidden_dim=50\n",
        "\n",
        "\t\tself.input_size=2 * embedding_size\n",
        "\n",
        "\t\tself.W1 = nn.Linear(self.input_size, self.hidden_dim)\n",
        "\t\tself.tanh=nn.Tanh()\n",
        "\t\tself.W2 = nn.Linear(self.hidden_dim, 1)\t\n",
        "\n",
        "\tdef scorer(self, batch_x, batch_m):\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tInput: a batch containing:\n",
        "\t\t\t-- batch_m [list of Mention objects]: mention to resolve.  batch_m[i] contains a single Mention\n",
        "\t\t\t-- batch_x [list of [list of Mention objects]]: candidate antecedents. batch_x[i] contains a list of candidate antecedents for mention batch_m[i]\n",
        "\n",
        "\t\tEach input batch is batched to contain the same number of candidate antecedents\n",
        "\n",
        "\t\tOutput: numpy matrix [batch_size, number_of_antecedents + 1, 1] containing scores for all antecedents\n",
        "\t\t\t-- for j < number_of_antecedents, output[i,j] contains the score of batch_x[i][j] being the correct antecedent for batch_m[i] \n",
        "\t\t\t-- for j == number_of_antecedents, output[i,j] = 0 (the score for batch_m[i] being linked to no antecedent)\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\t\n",
        "\t\tthis_batch_size=len(batch_x)\n",
        "\t\tnum_ants=len(batch_x[0])\n",
        "\n",
        "\t\t# get representations for mentions\n",
        "\t\tlastWordID=[]\n",
        "\n",
        "\t\tfor idx, mention in enumerate(batch_m):\n",
        "\t\t\tlastWordID.append(mention.sentence_ids[mention.sentence_end_idx])\n",
        "\t \n",
        "\n",
        "\t\t# torch.squeeze() and unsqueeze() --> unsqueeze adds a dummy dimension (at dimension passed in)\n",
        "\t\t# if you want to correctly concatenate, we add the dummy dimension \n",
        "\t\t# [this_batch_size, 1, embedding_size]\n",
        "\t\tmention_LW_embeddings=self.embeddings(torch.LongTensor(lastWordID).to(device)).unsqueeze(1)\n",
        "\n",
        "\t\t# get representations for antecedents\n",
        "\t\tantLastWords=[]\n",
        "\t\tfor idx in range(len(batch_x)):\n",
        "\t\t\tantWords=[]\n",
        "\t\t\tfor ant_idx, ant in enumerate(batch_x[idx]):\n",
        "\t\t\t\tantWords.append(ant.sentence_ids[ant.sentence_end_idx])\n",
        "\n",
        "\t\t\tantLastWords.append(antWords)\n",
        "\n",
        "\t\t# [this_batch_size, num_ants, embedding_size]\n",
        "\t\tantecedent_LW_embeddings=self.embeddings(torch.LongTensor(antLastWords).to(device))\n",
        "\n",
        "\t\t# We want to generate a score for each antecedent for each mention. However,\n",
        "\t\t# mention_LW_embeddings is [this_batch_size, 1, embedding_size] while,\n",
        "\t\t# antecedent_LW_embeddings is [this_batch_size, num_ants, embedding_size].\n",
        "\t\t# So let's make a bunch of copies of mention_LW_embeddings (one for each of its candidate antecedents)\n",
        "\n",
        "\t\t# [this_batch_size, num_ants, embedding_size]\n",
        "\t\tmention_LW_embeddings_copies=mention_LW_embeddings.expand_as(antecedent_LW_embeddings)\n",
        "\n",
        "\t\t# Now that they're the same size, we can concatenate them together into one big matrix\n",
        "\n",
        "\t\t# [this_batch_size, num_ants, (embedding_size + embedding_size)]\n",
        "\t\tall_features=torch.cat([mention_LW_embeddings_copies, antecedent_LW_embeddings], 2)\n",
        "\t\t\n",
        "\t\t# [this_batch_size, num_ants, 1]\n",
        "\t\tpreds=self.W2(self.tanh(self.W1(all_features))).squeeze(-1)\n",
        "\n",
        "\t\t# Let's fix the score for starting a new entity to be 0; all of the other scores for candidate antecedents will end up \n",
        "\t\t# being relative to that.\n",
        "\n",
        "\t\t# [this_batch_size, 1]\n",
        "\t\tzeros=torch.FloatTensor(np.zeros((this_batch_size, 1))).to(device)\n",
        "\n",
        "\t\t# [this_batch_size, num_ants + 1, 1]\t\t\n",
        "\t\tpreds=torch.cat((preds, zeros), 1)\n",
        "\n",
        "\t\treturn preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oPEwL-wQfyf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######### HELPER FUNCTION FOR TRAINING STARTS #########\n",
        "#########  DONT'T EDIT THIS SECTION OF CODE   #########\n",
        "def forward_predict(batch_x, batch_m, scoring_function):\n",
        "\n",
        "  this_batch_size=len(batch_x)\n",
        "  num_ants=len(batch_x[0])\n",
        "\n",
        "  # if this batch has no antecedents, then it must start a new entity\n",
        "  if num_ants == 0:\n",
        "    return torch.LongTensor([0]*this_batch_size)\n",
        "  \n",
        "  # get predictions\n",
        "  preds=scoring_function(batch_x, batch_m)\n",
        "\n",
        "  # \n",
        "  arg_sorts=torch.argsort(preds, descending=True, dim=1)\n",
        "  tops=arg_sorts[:,0]\n",
        "\n",
        "  return tops\n",
        "\n",
        "\n",
        "def forward_train(batch_x, batch_y, batch_m, scoring_function):\n",
        "\n",
        "  num_batch=len(batch_x)\n",
        "  num_ants=len(batch_x[0])\n",
        "\n",
        "  # if this batch has no candidate antecedents, then each mention must start a new entity so there is only only choice we could make (hence no loss)\n",
        "  if num_ants == 0:\n",
        "    return None\n",
        "\n",
        "  preds=scoring_function(batch_x, batch_m)\n",
        "  preds_sum=torch.logsumexp(preds, 1)\n",
        "\n",
        "  running_loss=None\n",
        "\n",
        "\n",
        "  for i in range(num_batch):\n",
        "\n",
        "    # optimize marginal log-likelihood of true antecedents\n",
        "    if batch_y[i].nelement() == 0:\n",
        "      golds_sum=0.\n",
        "    else:\n",
        "      golds=torch.index_select(preds[i], 0, batch_y[i])\n",
        "      golds_sum=torch.logsumexp(golds, 0)\n",
        "\n",
        "    diff=preds_sum[i]-golds_sum\n",
        "\n",
        "    running_loss = diff if running_loss is None else running_loss + diff\n",
        "\n",
        "  return running_loss\n",
        "\n",
        "def get_batches(X, Y, M, batchsize):\n",
        "  sizes={}\n",
        "  for i in range(len(M)):\n",
        "    size=len(X[i])\n",
        "    if size not in sizes:\n",
        "      sizes[size]=[]\n",
        "    sizes[size].append((X[i], Y[i], M[i]))\n",
        "\n",
        "  batches=[]\n",
        "\n",
        "  for size in sizes:\n",
        "    i=0\n",
        "    while (i < len(sizes[size])):\n",
        "\n",
        "      data=sizes[size][i:i+batchsize]\n",
        "      batch_x=[]\n",
        "      batch_y=[]\n",
        "      batch_m=[]\n",
        "      for x, y, m in data:\n",
        "        batch_x.append(x)\n",
        "        batch_y.append(y)\n",
        "        batch_m.append(m)\n",
        "\n",
        "      batches.append((batch_x, batch_y, batch_m))\n",
        "      i+=batchsize\n",
        "\n",
        "  return batches\n",
        "\n",
        "\n",
        "def train(X, Y, M, train_gold, test_X, test_Y, test_M, test_gold, model):\n",
        "\n",
        "  batches=get_batches(X, Y, M, 32)\n",
        "  test_batches=get_batches(test_X, test_Y, test_M, 32)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "  for epoch in range(10):\n",
        "\n",
        "    model.train()\n",
        "    # train\n",
        "    bigloss=0.\n",
        "    for batch_x, batch_y, batch_m in batches:\n",
        "      model.zero_grad()\n",
        "      loss=forward_train(batch_x, batch_y, batch_m, model.scorer)\n",
        "      if loss is not None:\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        bigloss+=loss\n",
        "\n",
        "    # evaluate\n",
        "    model.eval()\n",
        "\n",
        "    gold={}\n",
        "    predicted={}\n",
        "\n",
        "    eid=0\n",
        "    tot=0\n",
        "\n",
        "    for batch_x, batch_y, batch_m in test_batches:\n",
        "      predictions=forward_predict(batch_x, batch_m, model.scorer)\n",
        "\n",
        "      for idx, mention in enumerate(batch_m):\n",
        "\n",
        "        gold[mention.docid, mention.absolute_start_idx, mention.absolute_end_idx]=test_gold[mention.mention_id]\n",
        "        prediction=predictions[idx]\n",
        "        tot+=1\n",
        "      \n",
        "        # prediction is to start a new entity\n",
        "        if prediction >= len(batch_x[idx]):\n",
        "          predicted[mention.docid, mention.absolute_start_idx, mention.absolute_end_idx]=eid\n",
        "          eid+=1\n",
        "\n",
        "        # prediction is to link to a previous mention\n",
        "        else:\n",
        "\n",
        "          best_antecedent=batch_x[idx][prediction]\n",
        "          predicted_entity_id=predicted[best_antecedent.docid, best_antecedent.absolute_start_idx, best_antecedent.absolute_end_idx]\n",
        "          predicted[mention.docid, mention.absolute_start_idx, mention.absolute_end_idx]=predicted_entity_id\n",
        "\n",
        "    P, R, F=b3(gold, predicted)\n",
        "    print(\"loss: %.3f, B3 F: %.3f, unique entities: %s, num mentions: %s\" % (bigloss, F, eid, tot))\n",
        "\n",
        "def set_seed(seed):\n",
        "  \"\"\"\n",
        "  Sets random seeds and sets model in deterministic\n",
        "  training mode. Ensures reproducible results\n",
        "  \"\"\"\n",
        "  torch.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  np.random.seed(seed)\n",
        "######### HELPER FUNCTION FOR TRAINING ENDS #########\n",
        "#########  DONT'T EDIT THIS SECTION OF CODE   #########"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7_H0_p-bHhp",
        "colab_type": "text"
      },
      "source": [
        "Now, everything is set up to run the BasicCorefModel. Let's run the cell below to train the model and look at the result of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnvgFWdPblQ_",
        "colab_type": "code",
        "outputId": "54893406-460f-427d-ffe1-ababdf5179ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "X, Y, M, train_truth=convert_data_to_training_instances(all_sents, all_ents, vocab)\n",
        "dev_X, dev_Y, dev_M, dev_truth=convert_data_to_training_instances(dev_all_sents, dev_all_ents, vocab)\n",
        "model=BasicCorefModel(vocab, embeddings)\n",
        "model=model.to(device)\n",
        "print (\"Training BasicCorefModel\")\n",
        "set_seed(159)\n",
        "train(X, Y, M, train_truth, dev_X, dev_Y, dev_M, dev_truth, model)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training BasicCorefModel\n",
            "loss: 41274.824, B3 F: 0.764, unique entities: 29578, num mentions: 29597\n",
            "loss: 33196.121, B3 F: 0.764, unique entities: 29569, num mentions: 29597\n",
            "loss: 29578.076, B3 F: 0.765, unique entities: 29323, num mentions: 29597\n",
            "loss: 27773.998, B3 F: 0.771, unique entities: 28797, num mentions: 29597\n",
            "loss: 26788.645, B3 F: 0.779, unique entities: 27948, num mentions: 29597\n",
            "loss: 26151.428, B3 F: 0.782, unique entities: 27427, num mentions: 29597\n",
            "loss: 25682.391, B3 F: 0.783, unique entities: 27372, num mentions: 29597\n",
            "loss: 25319.143, B3 F: 0.786, unique entities: 26932, num mentions: 29597\n",
            "loss: 25024.688, B3 F: 0.789, unique entities: 26854, num mentions: 29597\n",
            "loss: 24776.920, B3 F: 0.793, unique entities: 26450, num mentions: 29597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi-A_Ho9P3bl",
        "colab_type": "text"
      },
      "source": [
        "### **Part 2.1 Incorporate distance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "718TVDuESx4G",
        "colab_type": "text"
      },
      "source": [
        "In this part, you should incorporate the word distance information to BasicCorefModel described in the HW. The below code structure provided to you is exactly the same as BasicCorefModel, your job is to add code into both __init__() and scorer() functions as you see fit.\n",
        "\n",
        "Hint: You might consider initialize distance embedding in __init__() function, then concatenate the original embedding and the corresponding distance embedding in scorer(). \n",
        "\n",
        "After implementing this, run the sanity check, test_distance(), provided to you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04w8HtOHzm25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DistanceCorefModel(nn.Module):\n",
        "\n",
        "\t\"\"\" The code provided here starts out as just a copy of BasicCorefModel \"\"\"\n",
        "\n",
        "\tdef __init__(self, vocab, embeddings):\n",
        "\t\tsuper(DistanceCorefModel, self).__init__()\n",
        "\n",
        "\t\tself.vocab=vocab\n",
        "\n",
        "\t\tself.embeddings = nn.Embedding.from_pretrained(embeddings)\n",
        "\n",
        "\t\t_, embedding_size=embeddings.shape\n",
        "\t\t## MY CODE ###\n",
        "\t\tself.dist = np.identity(10)\n",
        "\t\tself.d_embeddings = nn.Embedding.from_pretrained(torch.eye(10))\n",
        "\t\n",
        "\t\tself.hidden_dim=50\n",
        "\t\tself.input_size=2 * embedding_size + 10 #increase input size because of additional vector \n",
        "\t\t## END MY CODE ##\n",
        "\n",
        "\t\tself.W1 = nn.Linear(self.input_size, self.hidden_dim)\n",
        "\t\tself.tanh=nn.Tanh()\n",
        "\t\tself.W2 = nn.Linear(self.hidden_dim, 1)\t\n",
        "\n",
        "\tdef scorer(self, batch_x, batch_m):\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tInput: a batch containing:\n",
        "\t\t\t-- batch_m [list of Mention objects]: mention to resolve.  batch_m[i] contains a single Mention\n",
        "\t\t\t-- batch_x [list of [list of Mention objects]]: candidate antecedents. batch_x[i] contains a list of candidate antecedents for mention batch_m[i]\n",
        "\n",
        "\t\tEach input batch is batched to contain the same number of candidate antecedents\n",
        "\n",
        "\t\tOutput: numpy matrix [batch_size, number_of_antecedents + 1, 1] containing scores for all antecedents\n",
        "\t\t\t-- for j < number_of_antecedents, output[i,j] contains the score of batch_x[i][j] being the correct antecedent for batch_m[i] \n",
        "\t\t\t-- for j == number_of_antecedents, output[i,j] = 0 (the score for batch_m[i] being linked to no antecedent)\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tdevice = torch.device(\"cpu\")\n",
        "\n",
        "\t\tthis_batch_size=len(batch_x)\n",
        "\t\tnum_ants=len(batch_x[0])\n",
        "\t\n",
        "\t\tdistances = [] \n",
        "\t\t\n",
        "\t\tfor idx, ment in enumerate(batch_m):\n",
        "\t\t\tadist = []\n",
        "\t\t\tfor a in batch_x[idx]:\n",
        "\t\t\t\tdist = abs(ment.absolute_end_idx - a.absolute_end_idx)\n",
        "\t\t\t\tif dist <= 4:\n",
        "\t\t\t\t\tbucket = int(dist)\n",
        "\t\t\t\telif dist >= 5 and dist <= 7:\n",
        "\t\t\t\t\tbucket = 5\n",
        "\t\t\t\telif dist >= 8 and dist <= 15:\n",
        "\t\t\t\t\tbucket = 6\n",
        "\t\t\t\telif dist >= 16 and dist <= 31:\n",
        "\t\t\t\t\tbucket = 7\n",
        "\t\t\t\telif dist >= 32 and dist <= 63:\n",
        "\t\t\t\t\tbucket = 8\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tbucket = 9\n",
        "\t\t\t\tadist.append(bucket) \n",
        "\t\t\tdistances.append(adist)\n",
        "\t\n",
        "\t\td_embeddings = self.d_embeddings(torch.LongTensor(distances).to(device))\n",
        "\t\t#print(d_embeddings.shape)\n",
        "\t\t#print(d_embeddings)\n",
        "\t\n",
        "\t\t# get representations for mentions\n",
        "\t\tlastWordID=[]\n",
        "\n",
        "\t\tfor idx, mention in enumerate(batch_m):\n",
        "\t\t\tlastWordID.append(mention.sentence_ids[mention.sentence_end_idx])\n",
        "\n",
        "\t\t# [this_batch_size, 1, embedding_size]\n",
        "\t\tmention_LW_embeddings=self.embeddings(torch.LongTensor(lastWordID).to(device)).unsqueeze(1)\n",
        "\n",
        "\t\t# get representations for antecedents\n",
        "\t\tantLastWords=[]\n",
        "\t\tfor idx in range(len(batch_x)):\n",
        "\t\t\tantWords=[]\n",
        "\t\t\tfor ant_idx, ant in enumerate(batch_x[idx]):\n",
        "\t\t\t\tantWords.append(ant.sentence_ids[ant.sentence_end_idx])\n",
        "\t\t\t\t#get index of last word in that span of tokens, make it a torch, array of values, \n",
        "\t\t\t\t#pass it thru embeddings table, it becomes 1d tensor-->2dim \n",
        "\t\t\t\t\n",
        "\t\t\t\t#appending index of distance embedding ?\n",
        "\t\t\tantLastWords.append(antWords)\n",
        "\n",
        "\t\t# [this_batch_size, num_ants, embedding_size], it gets embeddings from the indexes \n",
        "\t\tantecedent_LW_embeddings=self.embeddings(torch.LongTensor(antLastWords).to(device))\n",
        "\n",
        "\t\t# We want to generate a score for each antecedent for each mention. However,\n",
        "\t\t# mention_LW_embeddings is [this_batch_size, 1, embedding_size] while,\n",
        "\t\t# antecedent_LW_embeddings is [this_batch_size, num_ants, embedding_size].\n",
        "\t\t# So let's make a bunch of copies of mention_LW_embeddings (one for each of its candidate antecedents)\n",
        "\n",
        "\t\t# [this_batch_size, num_ants, embedding_size]\n",
        "\t\tmention_LW_embeddings_copies=mention_LW_embeddings.expand_as(antecedent_LW_embeddings)\n",
        "\t\t# Now that they're the same size, we can concatenate them together into one big matrix\n",
        "\n",
        "\t\t\t#distance embedding table, same way we have word embedding table\n",
        "\t\t\t#declare that in model initiation funciton (trainable parameter) \n",
        "\t\t\t#in scorer, we index into this table #buckets * dimensionality of buckets \n",
        "\t\t\t#for each mi, we're given list of possible mjs\n",
        "\t\t\t#give each mi, mj pair a score \n",
        "\t#\tprint(mention_LW_embeddings_copies.shape)\n",
        "\t#\tprint(mention_LW_embeddings)\n",
        "\t#\tprint(antecedent_LW_embeddings.shape)\n",
        "\t#\tprint(antecedent_LW_embeddings)\n",
        "\t\t# [this_batch_size, num_ants, (embedding_size + embedding_size + distance_embedding_size)]\n",
        "\t\tall_features=torch.cat([mention_LW_embeddings_copies, antecedent_LW_embeddings, d_embeddings], 2)\n",
        "\n",
        "\t\t# [this_batch_size, num_ants, 1]\n",
        "\t\tpreds=self.W2(self.tanh(self.W1(all_features))).squeeze(-1)\n",
        "\n",
        "\t\t# Let's fix the score for starting a new entity to be 0; all of the other scores for candidate antecedents will end up \n",
        "\t\t# being relative to that.\n",
        "\n",
        "\t\t# [this_batch_size, 1]\n",
        "\t\tzeros=torch.FloatTensor(np.zeros((this_batch_size, 1))).to(device)\n",
        "\n",
        "\t\t# [this_batch_size, num_ants + 1, 1]\t\t\n",
        "\t\tpreds=torch.cat((preds, zeros), 1)\n",
        "\n",
        "\t\treturn preds\n",
        "\n",
        "\t\t#generate a bunch of candidate mentions\n",
        "\t\t#create a score that particular span is am ention\n",
        "\t\t#for each pair of mentions, also give score that confidence that mi coref mj\n",
        "\t\t# that's what scorer does\n",
        "\t\t#batch m is a list of candidate mentions, mis\n",
        "\t\t#batch x is a list of len(batchm) where each entry is all mentions that precede mi --> mjs for each mi\n",
        "\t\t#you get embedding for each mi,mj pari (for mi, mj, and distance between the two)\n",
        "\t\t#embedding between two, 1 by 10. but the whole table is 10 by 10 \n",
        "\t\t\n",
        "\n",
        "\t\t#say we have a mention mi, mj, distance = 17\n",
        "\t\t#pass 17 as a single parameter, but not that informative\n",
        "\t\t#we encode it as a vector. its dimensionality is arbitrary\n",
        "\t\t#we encode a continuous variable(distance) as categorical by bucketing \n",
        "\n",
        "\n",
        "\t\t#retrieve distance embedding from what you initialize \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iym9YsLOROHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_distance(model):\n",
        "  batch_x=[]\n",
        "  maxLen=100\n",
        "  for i in range(maxLen):\n",
        "    mention=Mention(i, \"testdoc\", i, i+1, 0, 1, [\"John\", \"Smith\", \"is\", \"a\", \"person\"], model.vocab)\n",
        "    batch_x.append(mention)\n",
        "\n",
        "  mention=Mention(maxLen, \"testdoc\", maxLen, maxLen, 0, 0, [\"He\", \"is\", \"a\", \"person\"], model.vocab)\n",
        "\n",
        "  preds=model.scorer([batch_x], [mention])\n",
        "  preds=preds.detach().cpu().numpy()[0]\n",
        "  spearman, _=spearmanr(preds, np.arange(len(preds)))\n",
        "  print(\"Distance check: %.3f\" % spearman)\n",
        "  with open(\"distance_predictions.txt\", \"w\", encoding=\"utf-8\") as out:\n",
        "    out.write(' '.join([\"%.5f\" % x for x in preds]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwVahwYxRVo-",
        "colab_type": "code",
        "outputId": "9fd6a132-71bd-425e-dc33-bbc0590de9bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "model=DistanceCorefModel(vocab, embeddings)\n",
        "model=model.to(device)\n",
        "\n",
        "print (\"Training DistanceCorefModel\")\n",
        "set_seed(159)\n",
        "train(X, Y, M, train_truth, dev_X, dev_Y, dev_M, dev_truth, model)\n",
        "test_distance(model)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training DistanceCorefModel\n",
            "loss: 38194.980, B3 F: 0.771, unique entities: 27554, num mentions: 29597\n",
            "loss: 31443.752, B3 F: 0.792, unique entities: 25700, num mentions: 29597\n",
            "loss: 27678.961, B3 F: 0.801, unique entities: 25084, num mentions: 29597\n",
            "loss: 25137.070, B3 F: 0.805, unique entities: 24554, num mentions: 29597\n",
            "loss: 23575.994, B3 F: 0.812, unique entities: 24162, num mentions: 29597\n",
            "loss: 22614.857, B3 F: 0.815, unique entities: 24009, num mentions: 29597\n",
            "loss: 21955.359, B3 F: 0.816, unique entities: 23907, num mentions: 29597\n",
            "loss: 21455.047, B3 F: 0.817, unique entities: 23822, num mentions: 29597\n",
            "loss: 21049.867, B3 F: 0.818, unique entities: 23730, num mentions: 29597\n",
            "loss: 20708.170, B3 F: 0.818, unique entities: 23622, num mentions: 29597\n",
            "Distance check: 0.921\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPy6ps0sSED9",
        "colab_type": "text"
      },
      "source": [
        "### **Part 2.2 Design a fancier model**\n",
        "Here comes the fun part! After completing DistanceCorefModel, you have certain degree of familiarity with the model architecture. In the section, you will be implementing a fancier model using any features you'd like. Feel free to make changes to the architecture you see fit.\n",
        "\n",
        "Submit this notebook to gradescope and a writeup file \"fancymodel.txt\" describing your model and the features you use.\n",
        "**Your code must implement exactly what you describe in your writeup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Tbubxvkzqoo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FancyCorefModel(nn.Module):\n",
        "\n",
        "\t\"\"\" The code provided here starts out as just a copy of BasicCorefModel \"\"\"\n",
        "\n",
        "\tdef __init__(self, vocab, embeddings):\n",
        "\t\tsuper(FancyCorefModel, self).__init__()\n",
        "\n",
        "\t\tself.vocab=vocab\n",
        "\n",
        "\t\tself.embeddings = nn.Embedding.from_pretrained(embeddings)\n",
        "\n",
        "\t\t_, embedding_size=embeddings.shape\n",
        "\n",
        "\t\tself.hidden_dim=50\n",
        "\n",
        "\t\tself.input_size=2 * embedding_size\n",
        "\n",
        "\t\tself.W1 = nn.Linear(self.input_size, self.hidden_dim)\n",
        "\t\tself.tanh=nn.Tanh()\n",
        "\t\tself.W2 = nn.Linear(self.hidden_dim, 1)\t\n",
        "\n",
        "\tdef scorer(self, batch_x, batch_m):\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\tInput: a batch containing:\n",
        "\t\t\t-- batch_m [list of Mention objects]: mention to resolve.  batch_m[i] contains a single Mention\n",
        "\t\t\t-- batch_x [list of [list of Mention objects]]: candidate antecedents. batch_x[i] contains a list of candidate antecedents for mention batch_m[i]\n",
        "\n",
        "\t\tEach input batch is batched to contain the same number of candidate antecedents\n",
        "\n",
        "\t\tOutput: numpy matrix [batch_size, number_of_antecedents + 1, 1] containing scores for all antecedents\n",
        "\t\t\t-- for j < number_of_antecedents, output[i,j] contains the score of batch_x[i][j] being the correct antecedent for batch_m[i] \n",
        "\t\t\t-- for j == number_of_antecedents, output[i,j] = 0 (the score for batch_m[i] being linked to no antecedent)\n",
        "\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tthis_batch_size=len(batch_x)\n",
        "\t\tnum_ants=len(batch_x[0])\n",
        "\n",
        "\t\t# get representations for mentions\n",
        "\t\tlastWordID=[]\n",
        "\n",
        "\t\tfor idx, mention in enumerate(batch_m):\n",
        "\t\t\tlastWordID.append(mention.sentence_ids[mention.sentence_end_idx])\n",
        "\n",
        "\t\t# [this_batch_size, 1, embedding_size]\n",
        "\t\tmention_LW_embeddings=self.embeddings(torch.LongTensor(lastWordID).to(device)).unsqueeze(1)\n",
        "\n",
        "\t\t# get representations for antecedents\n",
        "\t\tantLastWords=[]\n",
        "\t\tfor idx in range(len(batch_x)):\n",
        "\t\t\tantWords=[]\n",
        "\t\t\tfor ant_idx, ant in enumerate(batch_x[idx]):\n",
        "\t\t\t\tantWords.append(ant.sentence_ids[ant.sentence_end_idx])\n",
        "\t\t\t\t# append distance instead of sentence id \n",
        "\t\t\tantLastWords.append(antWords)\n",
        "\n",
        "\t\t# [this_batch_size, num_ants, embedding_size]\n",
        "\t\tantecedent_LW_embeddings=self.embeddings(torch.LongTensor(antLastWords).to(device))\n",
        "\n",
        "\t\t# We want to generate a score for each antecedent for each mention. However,\n",
        "\t\t# mention_LW_embeddings is [this_batch_size, 1, embedding_size] while,\n",
        "\t\t# antecedent_LW_embeddings is [this_batch_size, num_ants, embedding_size].\n",
        "\t\t# So let's make a bunch of copies of mention_LW_embeddings (one for each of its candidate antecedents)\n",
        "\n",
        "\t\t# [this_batch_size, num_ants, embedding_size]\n",
        "\t\tmention_LW_embeddings_copies=mention_LW_embeddings.expand_as(antecedent_LW_embeddings)\n",
        "\n",
        "\t\t# Now that they're the same size, we can concatenate them together into one big matrix\n",
        "\t\t\n",
        "\t\t# [this_batch_size, num_ants, (embedding_size + embedding_size)]\n",
        "\t\tall_features=torch.cat([mention_LW_embeddings_copies, antecedent_LW_embeddings], 2)\n",
        "\t\t\n",
        "\t\t# [this_batch_size, num_ants, 1]\n",
        "\t\tpreds=self.W2(self.tanh(self.W1(all_features))).squeeze(-1)\n",
        "\n",
        "\t\t# Let's fix the score for starting a new entity to be 0; all of the other scores for candidate antecedents will end up \n",
        "\t\t# being relative to that.\n",
        "\n",
        "\t\t# [this_batch_size, 1]\n",
        "\t\tzeros=torch.FloatTensor(np.zeros((this_batch_size, 1))).to(device)\n",
        "\n",
        "\t\t# [this_batch_size, num_ants + 1, 1]\t\t\n",
        "\t\tpreds=torch.cat((preds, zeros), 1)\n",
        "\n",
        "\t\treturn preds\n",
        "\n",
        "\t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uqdJ0085XoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=FancyCorefModel(vocab, embeddings)\n",
        "model=model.to(device)\n",
        "\n",
        "print (\"Training FancyCorefModel\")\n",
        "train(X, Y, M, train_truth, dev_X, dev_Y, dev_M, dev_truth, model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu73DIyWAEM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}